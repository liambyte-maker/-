# 决策树

决策树广泛运用与分类和回归问题

决策树，就是不断问问题，把数据分开。

节点（Node) :树中的每个点称为节点，根节点是树的节点，内部节点是决策点，叶节点是最终的决策结果

分支（Branch) 从一个节点到另一个节点的路径

分裂（split) 根据某个特征将数据集分为多个子集的过程

纯度（Purity) 衡量一个子集中样本的类别是否一致，纯度越高，说明子集的样本越相似。

## 决策树工作原理
1. 选择最佳特征，根据某种标准（信息增益，基尼指数等），选择最佳特征进行分割

2. 分割数据集，根据选定的特征将数据集分成多个子集

3. 递归构建字数，将每个字数重复上述过程，直到满足条件停止条件（如所有样本属于同一类别，达到最大深度等）

4.生成叶节点：当满足停止条件时，生成叶节点并赋予类别或值

## 决策树的构建标准

1. 信息增益（Information gain)

分裂前的混乱

<img width="676" height="163" alt="image" src="https://github.com/user-attachments/assets/b33664b9-ee2e-4219-a300-9f192d0226e0" />

分裂后的混乱
<img width="388" height="120" alt="image" src="https://github.com/user-attachments/assets/f1180239-dc3d-456d-97ab-b0a9a7352e94" />

分裂前的混乱-分裂后的混乱，如果非常大，说明把数据从非常乱，到很纯

2. 基尼指数（Gini index)

也是用于分类问题的分裂标准

<img width="371" height="124" alt="image" src="https://github.com/user-attachments/assets/0c11ea12-aee9-4477-8f5d-a70aff1f85b3" />

举例
<img width="438" height="205" alt="image" src="https://github.com/user-attachments/assets/f4149663-fee2-4d12-85ae-665f30a9472d" />

情况1，完全纯

<img width="500" height="309" alt="image" src="https://github.com/user-attachments/assets/f79c1e47-71a1-4193-b7f9-e9403fab5f9d" />

情况2，一半一半（最差）

<img width="466" height="283" alt="image" src="https://github.com/user-attachments/assets/4f9ea625-2355-43d6-963e-4446df9b46c2" />

情况3，偏向一边

衡量一个节点的纯度，越靠近0越纯

3. 均方误差(MSE)

MSE越小，表示回归树的预测效果越好


##决策树的优缺点

优点： 

易于解释和理解

处理多种数据类型，数值型和类别型

不需要数据标准化

缺点：

容易过拟合，特别是数据集特别小或者深度较大时

对噪音敏感，可能导致模型性能下降

不稳定，数据的小变化可能生成完全不同的树

```python

from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier()
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
```











