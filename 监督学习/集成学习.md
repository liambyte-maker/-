# 集成学习
Ensemble learning 是一种通过多个模型的预测结果来提高整体技能的技术

## Bagging:(bootstrap Aggregating)

1.随机抽样（有放回），可能有100个不同的数据集
2.训练多棵模型，每个子数据集训练一棵决策树
3.结果合并，多个基学习器的结果合并

典型算法，随机森林，通过构建多个决策树，每棵树在训练时随机选择特征，从而减少过拟合的风险

优势： 有效减少方差，提高稳定性
适用于高方差的模型，如决策树

缺点：训练时间过长，需要训练多个模型
结果难以解释，因为没有单一模型

## Boosting
通过减少模型的偏差来提高性能

有序化训练，模型是一个接一个训练的，每一轮都会根据前一轮的错误进行调整

加权投票

合并模型

梯度提升树 ：GBT通过迭代优化目标函数，逐步减少偏差

XGBoost 一种高效的梯度提升算法，广泛用于数据科学竞赛中

LightGBM, 相对于XGBoost,具有更快的训练速度和更低的内存使用

优势，适用于偏差较大的模型，提高预测准确性
强大的性能，在很多实际应用中表现优异

缺点：
对噪音比较敏感，容易过拟合

训练过程比较慢，特别是在数据量较大的情况下

## 3. Stacking
先让多个模型分别预测，再用一个更聪明的模型学会怎么组合他们

第一层，训练多个不同类型的基学习器
第二层，将第一次学习器的预测结果作为输入，训练一个元学习器



```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import StackingClassifier
```





