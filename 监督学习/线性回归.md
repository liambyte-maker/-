

##线性回归##

通过找到合适的w和b，使得误差最小

最小二乘法 least square method

<img width="307" height="101" alt="image" src="https://github.com/user-attachments/assets/48ffe7f0-97fd-4d2a-af9e-60d59ba77699" />

梯度下降法 gradient descent：

<img width="527" height="158" alt="image" src="https://github.com/user-attachments/assets/60a31680-2682-4122-b02d-5515053cbfb5" />

对w，b求偏导
更新w和b

<img width="479" height="239" alt="image" src="https://github.com/user-attachments/assets/b5c924ed-d237-47d5-894f-981f229b254b" />

1.初始化w和b的值

2.计算损失函数

3.计算梯度

4.根据梯度更新b和w

5.重复2到4，直到损失函数收敛或达到最大迭代次数

```python
import numpy as np
from sklearn.linear_model import LinearRegression
np.random.seed(0)
x=2*np.random.rand(100,1)    #0到1之间
y=4+3*x+np.randpm.randn(100,1)   #正态分布，均值=0，方差=1.误差通常近似正态分布
w=0
```




